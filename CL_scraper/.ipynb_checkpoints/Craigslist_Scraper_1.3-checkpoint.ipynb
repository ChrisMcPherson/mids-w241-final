{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Craigslist Apartment Scraper\n",
    "\n",
    "The purpose of this script is to pull apartment listings, characteristics, prices, and reply emails from for rent ads on craigslist. We plan to use this information in order to run an experiment to test the impact of including exclamation points on response rates to inquiries sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import xlsxwriter\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to query craigslist  \n",
    "\n",
    "This function will allow us to specify a price range, the number of bedrooms, and what craigslist site to query (e.g. Denver, SF, NYC, etc.)  \n",
    "\n",
    "Note that these queries only return a max of 100 results each. Thus, we will want to be specific about the price ranges and bedrooms that we specify so we can maximize the number of listings we are able to capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a function to fetch search results\n",
    "def fetch_search_results(query=None, minAsk=None, maxAsk=None, bedrooms=None, base_URL = None):\n",
    "    search_params = {key: val for key, val in locals().items() if val is not None}\n",
    "    if not search_params:\n",
    "        raise ValueError(\"No valid keywords\")\n",
    "    base = base_URL + '/search/apa'\n",
    "    resp = requests.get(base, params=search_params, timeout=3)\n",
    "    resp.raise_for_status()  # <- no-op if status==200\n",
    "    return resp.content, resp.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test the query function.\n",
    "test1, test2 = fetch_search_results(query = None, minAsk = 1000, maxAsk = 4000, bedrooms = 1, base_URL = 'https://denver.craigslist.org')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get full URLs and apartment characteristics from query function output  \n",
    "\n",
    "This function will go through each of the listings found from our query and compile a dataset of URLs and apartment characteristics of all the results from the query. We will use the URLs to get the reply email addresses in a later step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to get apartment characteristics from each query result  \n",
    "\n",
    "price, bedrooms, square footage, listing title, posting date / time, and reply linnk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get href - the relative link to the full apartment listing. These relative links are identified by <a> tags\n",
    "#and have the class 'result-title hdrlnk'.\n",
    "def get_href(result):\n",
    "    href = result.find('a', {'class' : 'result-title hdrlnk'})['href']\n",
    "    \n",
    "    if href is None:\n",
    "        href = np.nan\n",
    "    \n",
    "    return href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get posting ID - These IDs are the data-ID portion of  <a> tags with the class 'result-title hdrlnk'.\n",
    "def get_posting_ID(result):\n",
    "    posting_ID = result.find('a', {'class' : 'result-title hdrlnk'})['data-id']\n",
    "    \n",
    "    if posting_ID is None:\n",
    "        posting_ID = np.nan\n",
    "    \n",
    "    return posting_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get price - price can be located by <span> tags of class 'result-price'\n",
    "def get_price(result):\n",
    "    price = result.find('span', {'class' : 'result-price'})\n",
    "    \n",
    "    #convert price to float\n",
    "    if price is not None:\n",
    "        price = float(price.text.strip('$'))\n",
    "        \n",
    "    else:\n",
    "        price = np.nan\n",
    "    \n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get listing title which is identified by the text in the <a> tag with class 'result-title hdrlnk'\n",
    "def get_title(result):\n",
    "    title = result.find('a', {'class' : 'result-title hdrlnk'}).text\n",
    "    \n",
    "    if title is None:\n",
    "        title = np.nan\n",
    "        \n",
    "    return title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the time the listing was posted\n",
    "def get_posting_date(result):\n",
    "    posting_date = result.find('time', {'class' : 'result-date'})['datetime']\n",
    "    \n",
    "    if posting_date is None:\n",
    "        posting_date = np.nan\n",
    "        \n",
    "    return posting_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get bedrooms / sqft which is identified by the <span> tag of class 'housing'\n",
    "def get_bedrooms_sqft_str(result):\n",
    "    bedrooms_sqft = result.find('span', {'class' : 'housing'}).text.strip('\\n')\n",
    "    \n",
    "    if bedrooms_sqft is None:\n",
    "        price = np.nan\n",
    "    \n",
    "    return bedrooms_sqft\n",
    "\n",
    "def get_bedrooms_sqft(bedrooms_sqft):\n",
    "    #*******\n",
    "    #remove the new line characters and white space\n",
    "    p_1 = re.compile('-|\\n|\\s')\n",
    "\n",
    "    bedrooms_sqft = p_1.sub('', bedrooms_sqft)\n",
    "\n",
    "    #*******\n",
    "    #get bedrooms\n",
    "    #compile the regex\n",
    "    bedroom_p = re.compile(r'\\d+(?=br)', re.IGNORECASE)\n",
    "\n",
    "    #get match in the bedroom / sqft string\n",
    "    bedroom_m = bedroom_p.match(bedrooms_sqft)\n",
    "\n",
    "    #get bedrooms\n",
    "    n_bedrooms = float(bedrooms_sqft[bedroom_m.start(): bedroom_m.end()])\n",
    "\n",
    "    #*******\n",
    "    #get square footage\n",
    "    #remove bedrooms\n",
    "    bedrooms_sqft = bedrooms_sqft[bedroom_m.end() + 2:]\n",
    "\n",
    "    #compile the regex\n",
    "    sqft_p = re.compile(r'\\d+(?=ft)', re.IGNORECASE)\n",
    "\n",
    "    #get match in the square footage string\n",
    "    sqft_m = sqft_p.match(bedrooms_sqft)\n",
    "\n",
    "    #get square footage\n",
    "    try:\n",
    "        sqft = float(bedrooms_sqft[sqft_m.start():sqft_m.end()])\n",
    "    \n",
    "    except AttributeError:\n",
    "        sqft = np.nan\n",
    "    \n",
    "    return n_bedrooms, sqft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to compile all apartment characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compile_listing_URLs(query_result, base_URL):\n",
    "    #parse the results of the query\n",
    "    html = bs4(query_result, 'html.parser')\n",
    "\n",
    "    #get all individual apartments from the query\n",
    "    apt_results = html.find_all('p', attrs={'class' : 'result-info'})\n",
    "\n",
    "    #initialize a list to contain all of the URLs that resulted from the query\n",
    "    apts_results_df = pd.DataFrame(columns = ('base_URL', 'href','posting_ID', 'Listing_Title', 'Bedrooms', 'Sqft', 'Price', 'Posting_Date'))\n",
    "   \n",
    "    #Looop through all of the tags containing the apartments and get the addresses of those individual results.\n",
    "    for apt in range(len(apt_results)):\n",
    "        #use helper functions to get characteristics\n",
    "        href = get_href(apt_results[apt])\n",
    "        posting_ID = get_posting_ID(apt_results[apt])\n",
    "        title = get_title(apt_results[apt])\n",
    "        bedrooms_sqft_str = get_bedrooms_sqft_str(apt_results[apt])\n",
    "        bedrooms, sqft = get_bedrooms_sqft(bedrooms_sqft_str)\n",
    "        price = get_price(apt_results[apt])\n",
    "        posting_date = get_posting_date(apt_results[apt])\n",
    "        #populate the result dataframe with the characteristics\n",
    "        apts_results_df.loc[apt] = [base_URL, href, posting_ID, title, bedrooms, sqft, price, posting_date]\n",
    "\n",
    "    #construct full URL for the listing\n",
    "    apts_results_df['full_URL'] = apts_results_df.apply(lambda row: row['base_URL'] + row['href'], axis = 1)\n",
    "    \n",
    "    #construct reply URL for the listing\n",
    "    apts_results_df['Reply_contact_info_link'] = apts_results_df.apply(lambda row: row['base_URL'] + '/reply/den/apa/' + row['posting_ID'].strip('.html'), axis = 1)\n",
    "    \n",
    "    #delete base URL and href columns\n",
    "    del apts_results_df['base_URL']\n",
    "    del apts_results_df['href']\n",
    "    \n",
    "    return apts_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test the compiler function\n",
    "test_compiled_URLs = compile_listing_URLs(query_result = test1, base_URL = 'https://denver.craigslist.org')\n",
    "\n",
    "test_compiled_URLs.head()\n",
    "\n",
    "pd.set_option('display.max_colwidth',1000)\n",
    "\n",
    "test_compiled_URLs.head()\n",
    "\n",
    "os.chdir('/Users/nwchen24/Desktop/UC_Berkeley/Experiments_and_causality/final_project_github_repo/mids-w241-final/CL_scraper/output')\n",
    "\n",
    "#write the output file\n",
    "writer = pd.ExcelWriter('Example Listing Pull for Denver 2-20-17.xlsx', engine = 'xlsxwriter')     \n",
    "test_compiled_URLs.to_excel(writer, sheet_name = 'Example Listings')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operationalizing Phase  \n",
    "\n",
    "This phase will incorporate the ability to run the scraper across a selection of cities and bedroom and price range specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City to Craigslist URLs Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://denver.craigslist.org\n",
      "https://newyork.craigslist.org\n",
      "https://cleveland.craigslist.org\n",
      "https://sfbay.craigslist.org\n"
     ]
    }
   ],
   "source": [
    "#create city list\n",
    "cities = ['denver', 'newyork', 'cleveland', 'sanfrancisco']\n",
    "\n",
    "#set base craigslist URLs\n",
    "base_URLs = ['https://denver.craigslist.org', 'https://newyork.craigslist.org', 'https://cleveland.craigslist.org', \\\n",
    "             'https://sfbay.craigslist.org']\n",
    "\n",
    "#set search URLS to feed to query function\n",
    "search_URLs = ['https://denver.craigslist.org/search/apa', 'https://newyork.craigslist.org/search/abo', \\\n",
    "               'https://cleveland.craigslist.org/search/apa', 'https://sfbay.craigslist.org/search/sfc/apa']\n",
    "\n",
    "#set reply strings which are intermediate strings between the base URL and the posting ID to access the page where\n",
    "#reply emails are found\n",
    "reply_strings = ['/reply/den/apa/', '/reply/nyc/abo/', '/reply/cle/apa/', '/reply/sfo/apa/']\n",
    "\n",
    "#create dataframe with all of these pieces of information\n",
    "city_to_URL_dict = {'base_URL' : base_URLs, 'search_URL' : search_URLs, 'reply_string' : reply_strings}\n",
    "\n",
    "city_to_URL_df = pd.DataFrame(city_to_URL_dict, index = cities)\n",
    "\n",
    "for city in city_to_URL_df.index:\n",
    "    print city_to_URL_df.loc[city, 'base_URL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NOTE LEFT OFF HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline function to pull data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Success. City: denver Bedrooms: 1 Price Range: 500-1000\n"
     ]
    }
   ],
   "source": [
    "#Look at $500 price bucket increments for each number of bedrooms\n",
    "min_prices = np.arange(500, 5000, 500).tolist()\n",
    "\n",
    "#Look at 1, 2, 3 bedroom apartments\n",
    "bedrooms = [1,2,3]\n",
    "\n",
    "#initialize empty dataframe to hold query results\n",
    "query_results_df = pd.DataFrame(columns = ('city', 'min_price', 'max_price', 'bedrooms', 'query_html'))\n",
    "\n",
    "#initialize counter which we will use to populate the query result dataframe\n",
    "counter = 0\n",
    "\n",
    "#loop over cities\n",
    "for city in city_to_URL_df.index:\n",
    "    #loop over number of bedrooms\n",
    "    for bedroom in bedrooms:\n",
    "        #loop over the min prices\n",
    "        for price in min_prices:\n",
    "            #set start time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            #ping CL server to get query results\n",
    "            query_results, query_encoding = fetch_search_results(query = None, minAsk = price, maxAsk = price + 500, bedrooms = bedroom,\\\n",
    "                                 base_URL = city_to_URL_df.loc[city, 'base_URL'])\n",
    "            \n",
    "            if query_results is not None:\n",
    "                print \"Query Success. City: \" + city + \", Bedrooms: \" + str(bedroom) + \", Price Range: \" + str(price) +\\\n",
    "                \"-\" + str(price + 500)            \n",
    "            \n",
    "            else:\n",
    "                print \"No Results Found. City: \" + city + \", Bedrooms: \" + str(bedroom) + \", Price Range: \" + str(price) +\\\n",
    "                \"-\" + str(price + 500)            \n",
    "           \n",
    "            \n",
    "            #append result (which is a string) to query results dataframe\n",
    "            query_results_df.loc[counter] = [city, price, price + 500, bedroom, query_results]\n",
    "            \n",
    "            #increment counter\n",
    "            counter += 1\n",
    "            \n",
    "            #incorporate delay drawn from normal distribution centered around one minute to hopefully make\n",
    "            #queries appear more human like\n",
    "            delay = abs(np.random.normal(60, 15))\n",
    "            time.sleep(delay)\n",
    "            print \"Delay: \" + str(time.time() - start_time)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Delay: 4.34614300728\n",
      "2\n",
      "Delay: 2.38538599014\n",
      "3\n",
      "Delay: 5.01460599899\n",
      "4\n",
      "Delay: 3.70861887932\n",
      "5\n",
      "Delay: 3.37760400772\n"
     ]
    }
   ],
   "source": [
    "#outline for including a delay in queries\n",
    "for i in [1, 2, 3, 4, 5]:\n",
    "    start_time = time.time()\n",
    "    print i\n",
    "    delay = abs(np.random.normal(5, 1))\n",
    "    time.sleep(delay)\n",
    "    print \"Delay: \" + str(time.time() - start_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Machine_learning_python2]",
   "language": "python",
   "name": "conda-env-Machine_learning_python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
