{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Craigslist Apartment Scraper\n",
    "\n",
    "The purpose of this script is to pull apartment listings, characteristics, prices, and reply emails from for rent ads on craigslist. We plan to use this information in order to run an experiment to test the impact of including exclamation points on response rates to inquiries sent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version Notes  \n",
    "- This version does not include any testing code cells. For testing code cells, refer to v1.5\n",
    "- Unless you wish to pull new listings, be careful not to run the cell that runs queries in all four cities for the bedroom and price combinations specified\n",
    "- This version pulls listings from Seattle, Houston, Atlanta, and Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import xlsxwriter\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Set directory for input / output\n",
    "\n",
    "#DIRECTORY FOR HOME MACHINE\n",
    "os.chdir('/Users/nwchen24/Desktop/UC_Berkeley/Experiments_and_causality/final_project_github_repo/mids-w241-final/CL_scraper/output')\n",
    "\n",
    "#DIRECTORY FOR WORK MACHINE\n",
    "#os.chdir('C:/Users/nchen/Desktop/Nick/UC_Berkeley/experiments_and_causality/final_project_github_repo/mids-w241-final/CL_scraper/output')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to query craigslist  \n",
    "\n",
    "This function will allow us to specify a price range, the number of bedrooms, and what craigslist site to query (e.g. Denver, SF, NYC, etc.)  \n",
    "\n",
    "Note that these queries only return a max of 100 results each. Thus, we will want to be specific about the price ranges and bedrooms that we specify so we can maximize the number of listings we are able to capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a function to fetch search results\n",
    "def fetch_search_results(query=None, minAsk=None, maxAsk=None, bedrooms=None, search_URL = None):\n",
    "    search_params = {key: val for key, val in locals().items() if val is not None}\n",
    "    if not search_params:\n",
    "        raise ValueError(\"No valid keywords\")\n",
    "    resp = requests.get(search_URL, params=search_params, timeout=3)\n",
    "    resp.raise_for_status()  # <- no-op if status==200\n",
    "    return resp.content, resp.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get full URLs and apartment characteristics from query function output  \n",
    "\n",
    "This function will go through each of the listings found from our query and compile a dataset of URLs and apartment characteristics of all the results from the query. We will use the URLs to get the reply email addresses in a later step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to get apartment characteristics from each query result  \n",
    "\n",
    "price, bedrooms, square footage, listing title, posting date / time, and reply linnk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get href - the relative link to the full apartment listing. These relative links are identified by <a> tags\n",
    "#and have the class 'result-title hdrlnk'.\n",
    "def get_href(result):\n",
    "    href = result.find('a', {'class' : 'result-title hdrlnk'})['href']\n",
    "    \n",
    "    if href is None:\n",
    "        href = np.nan\n",
    "    \n",
    "    return href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get posting ID - These IDs are the data-ID portion of  <a> tags with the class 'result-title hdrlnk'.\n",
    "def get_posting_ID(result):\n",
    "    posting_ID = result.find('a', {'class' : 'result-title hdrlnk'})['data-id']\n",
    "    \n",
    "    if posting_ID is None:\n",
    "        posting_ID = np.nan\n",
    "    \n",
    "    return posting_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get price - price can be located by <span> tags of class 'result-price'\n",
    "def get_price(result):\n",
    "    price = result.find('span', {'class' : 'result-price'})\n",
    "    \n",
    "    #convert price to float\n",
    "    if price is not None:\n",
    "        price = float(price.text.strip('$'))\n",
    "        \n",
    "    else:\n",
    "        price = np.nan\n",
    "    \n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get listing title which is identified by the text in the <a> tag with class 'result-title hdrlnk'\n",
    "def get_title(result):\n",
    "    title = result.find('a', {'class' : 'result-title hdrlnk'}).text\n",
    "    \n",
    "    if title is None:\n",
    "        title = np.nan\n",
    "        \n",
    "    return title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the time the listing was posted\n",
    "def get_posting_date(result):\n",
    "    posting_date = result.find('time', {'class' : 'result-date'})['datetime']\n",
    "    \n",
    "    if posting_date is None:\n",
    "        posting_date = np.nan\n",
    "        \n",
    "    return posting_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get bedrooms / sqft which is identified by the <span> tag of class 'housing'\n",
    "def get_bedrooms_sqft_str(result):\n",
    "    bedrooms_sqft = result.find('span', {'class' : 'housing'}).text.strip('\\n')\n",
    "    \n",
    "    if bedrooms_sqft is None:\n",
    "        price = np.nan\n",
    "    \n",
    "    return bedrooms_sqft\n",
    "\n",
    "def get_bedrooms_sqft(bedrooms_sqft):\n",
    "    #*******\n",
    "    #remove the new line characters and white space\n",
    "    p_1 = re.compile('-|\\n|\\s')\n",
    "\n",
    "    bedrooms_sqft = p_1.sub('', bedrooms_sqft)\n",
    "\n",
    "    #*******\n",
    "    #get bedrooms\n",
    "    #compile the regex\n",
    "    bedroom_p = re.compile(r'\\d+(?=br)', re.IGNORECASE)\n",
    "\n",
    "    #get match in the bedroom / sqft string\n",
    "    bedroom_m = bedroom_p.match(bedrooms_sqft)\n",
    "\n",
    "    #get bedrooms\n",
    "    n_bedrooms = float(bedrooms_sqft[bedroom_m.start(): bedroom_m.end()])\n",
    "\n",
    "    #*******\n",
    "    #get square footage\n",
    "    #remove bedrooms\n",
    "    bedrooms_sqft = bedrooms_sqft[bedroom_m.end() + 2:]\n",
    "\n",
    "    #compile the regex\n",
    "    sqft_p = re.compile(r'\\d+(?=ft)', re.IGNORECASE)\n",
    "\n",
    "    #get match in the square footage string\n",
    "    sqft_m = sqft_p.match(bedrooms_sqft)\n",
    "\n",
    "    #get square footage\n",
    "    try:\n",
    "        sqft = float(bedrooms_sqft[sqft_m.start():sqft_m.end()])\n",
    "    \n",
    "    except AttributeError:\n",
    "        sqft = np.nan\n",
    "    \n",
    "    return n_bedrooms, sqft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to compile all apartment characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compile_listing_URLs(query_result, base_URL, reply_string, city):\n",
    "    #parse the results of the query\n",
    "    html = bs4(query_result, 'html.parser')\n",
    "\n",
    "    #get all individual apartments from the query\n",
    "    apt_results = html.find_all('p', attrs={'class' : 'result-info'})\n",
    "\n",
    "    #initialize a list to contain all of the URLs that resulted from the query\n",
    "    apts_results_df = pd.DataFrame(columns = ('city', 'base_URL', 'href','posting_ID', 'Listing_Title', 'Bedrooms', 'Sqft', 'Price', 'Posting_Date'))\n",
    "   \n",
    "    #Looop through all of the tags containing the apartments and get the addresses of those individual results.\n",
    "    for apt in range(len(apt_results)):\n",
    "        #use helper functions to get characteristics\n",
    "        href = get_href(apt_results[apt])\n",
    "        posting_ID = get_posting_ID(apt_results[apt])\n",
    "        title = get_title(apt_results[apt])\n",
    "        bedrooms_sqft_str = get_bedrooms_sqft_str(apt_results[apt])\n",
    "        bedrooms, sqft = get_bedrooms_sqft(bedrooms_sqft_str)\n",
    "        price = get_price(apt_results[apt])\n",
    "        posting_date = get_posting_date(apt_results[apt])\n",
    "        #populate the result dataframe with the characteristics\n",
    "        apts_results_df.loc[apt] = [city, base_URL, href, posting_ID, title, bedrooms, sqft, price, posting_date]\n",
    "\n",
    "    #construct full URL for the listing\n",
    "    apts_results_df['full_URL'] = apts_results_df.apply(lambda row: row['base_URL'] + row['href'], axis = 1)\n",
    "    \n",
    "    #construct reply URL for the listing\n",
    "    apts_results_df['Reply_contact_info_link'] = apts_results_df.apply(lambda row: row['base_URL'] + reply_string + row['posting_ID'].strip('.html'), axis = 1)\n",
    "    \n",
    "    #delete base URL and href columns\n",
    "    del apts_results_df['base_URL']\n",
    "    del apts_results_df['href']\n",
    "    \n",
    "    return apts_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operationalizing Phase  \n",
    "\n",
    "This phase will incorporate the ability to run the scraper across a selection of cities and bedroom and price range specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City to Craigslist URLs Dictionary  \n",
    "- If we want to expand the scope of our project to additional cities, this is the place to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create city list\n",
    "cities = ['seattle', 'houston', 'chicago', 'sandiego']\n",
    "\n",
    "#set base craigslist URLs\n",
    "base_URLs = ['https://seattle.craigslist.org', 'https://houston.craigslist.org', 'https://chicago.craigslist.org', \\\n",
    "             'https://sandiego.craigslist.org']\n",
    "\n",
    "#set search URLS to feed to query function\n",
    "search_URLs = ['https://seattle.craigslist.org/search/apa', 'https://houston.craigslist.org/search/apa', \\\n",
    "               'https://chicago.craigslist.org/search/apa', 'https://sandiego.craigslist.org/search/apa']\n",
    "\n",
    "#set reply strings which are intermediate strings between the base URL and the posting ID to access the page where\n",
    "#reply emails are found\n",
    "reply_strings = ['/reply/sea/apa/', '/reply/hou/apa/', '/reply/chi/apa/', '/reply/sdo/apa/']\n",
    "\n",
    "#create dataframe with all of these pieces of information\n",
    "city_to_URL_dict = {'base_URL' : base_URLs, 'search_URL' : search_URLs, 'reply_string' : reply_strings}\n",
    "\n",
    "city_to_URL_df = pd.DataFrame(city_to_URL_dict, index = cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull data for bedroom, city, price combinations - DO NOT RUN UNLESS YOU ACTUALLY WANT TO PULL NEW LISTINGS \n",
    "\n",
    "- Export results for each city to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Success. City: seattle, Bedrooms: 1, Price Range: 1000-1500\n",
      "Delay: 26.3669998646\n",
      "Query Success. City: seattle, Bedrooms: 1, Price Range: 1500-2000\n",
      "Delay: 37.0970001221\n",
      "Query Success. City: seattle, Bedrooms: 1, Price Range: 2000-2500\n",
      "Delay: 29.3519999981\n",
      "Query Success. City: seattle, Bedrooms: 1, Price Range: 2500-3000\n",
      "Delay: 31.7620000839\n",
      "Query Success. City: seattle, Bedrooms: 1, Price Range: 3000-3500\n",
      "Delay: 29.7669999599\n",
      "Query Success. City: seattle, Bedrooms: 1, Price Range: 3500-4000\n",
      "Delay: 30.507999897\n",
      "Query Success. City: seattle, Bedrooms: 2, Price Range: 1000-1500\n",
      "Delay: 4.74100017548\n",
      "Query Success. City: seattle, Bedrooms: 2, Price Range: 1500-2000\n",
      "Delay: 26.8480000496\n",
      "Query Success. City: seattle, Bedrooms: 2, Price Range: 2000-2500\n",
      "Delay: 12.2559998035\n",
      "Query Success. City: seattle, Bedrooms: 2, Price Range: 2500-3000\n",
      "Delay: 50.1950001717\n",
      "Query Success. City: seattle, Bedrooms: 2, Price Range: 3000-3500\n",
      "Delay: 18.128000021\n",
      "Query Success. City: seattle, Bedrooms: 2, Price Range: 3500-4000\n",
      "Delay: 24.7449998856\n",
      "Query Success. City: houston, Bedrooms: 1, Price Range: 1000-1500\n",
      "Delay: 20.0640001297\n",
      "Query Success. City: houston, Bedrooms: 1, Price Range: 1500-2000\n",
      "Delay: 12.8389999866\n",
      "Query Success. City: houston, Bedrooms: 1, Price Range: 2000-2500\n",
      "Delay: 22.1749999523\n",
      "Query Success. City: houston, Bedrooms: 1, Price Range: 2500-3000\n",
      "Delay: 19.6239998341\n",
      "Query Success. City: houston, Bedrooms: 1, Price Range: 3000-3500\n",
      "Delay: 24.631000042\n",
      "Query Success. City: houston, Bedrooms: 1, Price Range: 3500-4000\n",
      "Delay: 28.5420000553\n",
      "Query Success. City: houston, Bedrooms: 2, Price Range: 1000-1500\n",
      "Delay: 36.6909999847\n",
      "Query Success. City: houston, Bedrooms: 2, Price Range: 1500-2000\n",
      "Delay: 33.2920000553\n",
      "Query Success. City: houston, Bedrooms: 2, Price Range: 2000-2500\n",
      "Delay: 24.507999897\n",
      "Query Success. City: houston, Bedrooms: 2, Price Range: 2500-3000\n",
      "Delay: 22.375\n",
      "Query Success. City: houston, Bedrooms: 2, Price Range: 3000-3500\n",
      "Delay: 11.5190000534\n",
      "Query Success. City: houston, Bedrooms: 2, Price Range: 3500-4000\n",
      "Delay: 23.760999918\n",
      "Query Success. City: chicago, Bedrooms: 1, Price Range: 1000-1500\n",
      "Delay: 26.2660000324\n",
      "Query Success. City: chicago, Bedrooms: 1, Price Range: 1500-2000\n",
      "Delay: 23.8619999886\n",
      "Query Success. City: chicago, Bedrooms: 1, Price Range: 2000-2500\n",
      "Delay: 47.5550000668\n",
      "Query Success. City: chicago, Bedrooms: 1, Price Range: 2500-3000\n",
      "Delay: 18.0859999657\n",
      "Query Success. City: chicago, Bedrooms: 1, Price Range: 3000-3500\n",
      "Delay: 26.5320000648\n",
      "Query Success. City: chicago, Bedrooms: 1, Price Range: 3500-4000\n",
      "Delay: 28.8499999046\n",
      "Query Success. City: chicago, Bedrooms: 2, Price Range: 1000-1500\n",
      "Delay: 19.6579999924\n",
      "Query Success. City: chicago, Bedrooms: 2, Price Range: 1500-2000\n",
      "Delay: 46.4000000954\n",
      "Query Success. City: chicago, Bedrooms: 2, Price Range: 2000-2500\n",
      "Delay: 51.4859998226\n",
      "Query Success. City: chicago, Bedrooms: 2, Price Range: 2500-3000\n",
      "Delay: 29.3550000191\n",
      "Query Success. City: chicago, Bedrooms: 2, Price Range: 3000-3500\n",
      "Delay: 34.1400001049\n",
      "Query Success. City: chicago, Bedrooms: 2, Price Range: 3500-4000\n",
      "Delay: 18.0320000648\n",
      "Query Success. City: sandiego, Bedrooms: 1, Price Range: 1000-1500\n",
      "Delay: 22.8629999161\n",
      "Query Success. City: sandiego, Bedrooms: 1, Price Range: 1500-2000\n",
      "Delay: 40.4719998837\n",
      "Query Success. City: sandiego, Bedrooms: 1, Price Range: 2000-2500\n",
      "Delay: 23.2840001583\n",
      "Query Success. City: sandiego, Bedrooms: 1, Price Range: 2500-3000\n",
      "Delay: 29.9700000286\n",
      "Query Success. City: sandiego, Bedrooms: 1, Price Range: 3000-3500\n",
      "Delay: 32.6819999218\n",
      "Query Success. City: sandiego, Bedrooms: 1, Price Range: 3500-4000\n",
      "Delay: 23.503000021\n",
      "Query Success. City: sandiego, Bedrooms: 2, Price Range: 1000-1500\n",
      "Delay: 17.2650001049\n",
      "Query Success. City: sandiego, Bedrooms: 2, Price Range: 1500-2000\n",
      "Delay: 43.9600000381\n",
      "Query Success. City: sandiego, Bedrooms: 2, Price Range: 2000-2500\n",
      "Delay: 22.9919998646\n",
      "Query Success. City: sandiego, Bedrooms: 2, Price Range: 2500-3000\n",
      "Delay: 24.1290001869\n",
      "Query Success. City: sandiego, Bedrooms: 2, Price Range: 3000-3500\n",
      "Delay: 22.2809998989\n",
      "Query Success. City: sandiego, Bedrooms: 2, Price Range: 3500-4000\n",
      "Delay: 33.4000000954\n"
     ]
    }
   ],
   "source": [
    "#Look at $500 price bucket increments for each number of bedrooms\n",
    "min_prices = np.arange(1000, 4000, 500).tolist()\n",
    "\n",
    "#Look at 1, 2, 3 bedroom apartments\n",
    "bedrooms = [1, 2]\n",
    "\n",
    "#initialize counter which we will use to populate the query result dataframe\n",
    "counter = 0\n",
    "\n",
    "#loop over cities\n",
    "for city in city_to_URL_df.index:\n",
    "    #initialize empty dataframe to hold query results\n",
    "    query_results_df = pd.DataFrame(columns = ('city', 'min_price', 'max_price', 'bedrooms', 'query_html'))\n",
    "\n",
    "    #loop over number of bedrooms\n",
    "    for bedroom in bedrooms:\n",
    "        #loop over the min prices\n",
    "        for price in min_prices:\n",
    "            #set start time\n",
    "            start_time = time.time()\n",
    "\n",
    "            #ping CL server to get query results\n",
    "            query_results, query_encoding = fetch_search_results(query = None, minAsk = price, maxAsk = price + 500, bedrooms = bedroom,\\\n",
    "                                 search_URL = city_to_URL_df.loc[city, 'search_URL'])\n",
    "\n",
    "            if query_results is not None:\n",
    "                print \"Query Success. City: \" + city + \", Bedrooms: \" + str(bedroom) + \", Price Range: \" + str(price) +\\\n",
    "                \"-\" + str(price + 500)            \n",
    "\n",
    "            else:\n",
    "                print \"No Results Found. City: \" + city + \", Bedrooms: \" + str(bedroom) + \", Price Range: \" + str(price) +\\\n",
    "                \"-\" + str(price + 500)            \n",
    "\n",
    "\n",
    "            #append result (which is a string) to query results dataframe\n",
    "            query_results_df.loc[counter] = [city, price, price + 500, bedroom, query_results]\n",
    "\n",
    "            #increment counter\n",
    "            counter += 1\n",
    "\n",
    "            #incorporate delay drawn from normal distribution centered around one minute to hopefully make\n",
    "            #queries appear more human like\n",
    "            delay = abs(np.random.normal(25, 10))\n",
    "            time.sleep(delay)\n",
    "            print \"Delay: \" + str(time.time() - start_time)\n",
    "\n",
    "    #write the results for each city to csv\n",
    "    query_results_df.to_csv(city + '_query_results_output_3-17-17.csv', sep='\\t', encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get listing information we need from the query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First, read the listings back from csv\n",
    "file_names = []\n",
    "\n",
    "#compile file names to read\n",
    "for city in city_to_URL_df.index:\n",
    "    file_names.append(city + '_query_results_output_3-17-17.csv')\n",
    "\n",
    "#initialize query results df\n",
    "query_results_comb_df = pd.DataFrame(columns = ('Unnamed: 0', 'city', 'min_price', 'max_price', 'bedrooms', 'query_html'))\n",
    "\n",
    "#read each output file for each city and append to a combined dataframe\n",
    "for filename in file_names:\n",
    "    query_results_comb_df = query_results_comb_df.append(pd.read_csv(filename, sep='\\t', encoding='utf-8'))\n",
    "\n",
    "#delete columns we don't want and reset index\n",
    "del query_results_comb_df['Unnamed: 0']\n",
    "query_results_comb_df = query_results_comb_df.reset_index()\n",
    "del query_results_comb_df['index']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through combined dataframe to get individual listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize listing results df\n",
    "combined_listings_df = pd.DataFrame(columns = ('city', 'posting_ID', 'Listing_Title', 'Bedrooms', 'Sqft', 'Price', 'Posting_Date'))\n",
    "\n",
    "#loop through query results\n",
    "for i in range(query_results_comb_df.shape[0]):\n",
    "    #get city\n",
    "    city_to_use = query_results_comb_df.loc[i, 'city']\n",
    "    #get the intermediate reply string to feed to function that gets individual listings\n",
    "    reply_string_to_use = city_to_URL_df.loc[city_to_use, 'reply_string']\n",
    "    #get base URL to feed to function that gets individual listings\n",
    "    base_URL_to_use = city_to_URL_df.loc[city_to_use, 'base_URL']\n",
    "    #get the query result html code to feed to function that gets individual listings\n",
    "    query_result_html = query_results_comb_df.loc[i, 'query_html']\n",
    "    \n",
    "    #Get individual listings from query results\n",
    "    query_results_df_intermed = compile_listing_URLs(query_result = query_result_html, base_URL = base_URL_to_use,\\\n",
    "                                                     city = city_to_use, reply_string = reply_string_to_use)\n",
    "    \n",
    "    #append the results for each bedroom, city, price combination to the combined dataframe\n",
    "    combined_listings_df = combined_listings_df.append(query_results_df_intermed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save combined listing dataset to csv\n",
    "#re-order columns\n",
    "combined_listings_df = combined_listings_df[['Listing_Title', 'Posting_Date', 'city', 'posting_ID', 'full_URL', 'Bedrooms',\\\n",
    "                                            'Sqft', 'Price', 'Reply_contact_info_link']]\n",
    "\n",
    "#Add a column where mechanical turk can fill in reply email address\n",
    "combined_listings_df['reply_email_TO_BE_FILLED_IN'] = ''\n",
    "\n",
    "#export to CSV\n",
    "combined_listings_df.to_csv('Combined Scraper Listing Results 3-17-17.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5760, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_listings_df.head()\n",
    "\n",
    "combined_listings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 12)\n"
     ]
    }
   ],
   "source": [
    "#Select a random sample from the query results.\n",
    "#Also assign each to treatment group\n",
    "\n",
    "#First, initialize the dataframe to hold the results\n",
    "test_sample_df = pd.DataFrame(columns = ('city', 'Listing_Title', 'Posting_Date', 'Bedrooms', 'Sqft', 'Price',\\\n",
    "                           'posting_ID', 'treatment_assignment', 'full_URL', 'Reply_contact_info_link',\\\n",
    "                                         'reply_email_TO_BE_FILLED_IN', 'Apartment_Complex_Flag'))\n",
    "#create vector of possible treatment assignments\n",
    "treatments = ['John_Control',  'John_Treat_Low',  'John_Treat_High', 'Jane_Control',  'Jane_Treat_Low',  'Jane_Treat_High']\n",
    "\n",
    "for city in cities:\n",
    "    #Instantiate city DF\n",
    "    city_df = pd.DataFrame(columns = ('city', 'Listing_Title', 'Posting_Date', 'Bedrooms', 'Sqft', 'Price',\\\n",
    "                           'posting_ID', 'treatment_assignment', 'full_URL', 'Reply_contact_info_link',\\\n",
    "                                         'reply_email_TO_BE_FILLED_IN', 'Apartment_Complex_Flag'))\n",
    "\n",
    "    for bedroom in [1,2]:\n",
    "        #Take a sample of five listings for each city and bedroom combination\n",
    "        intermed_df = combined_listings_df.loc[(combined_listings_df.city == city)\\\n",
    "                                               & (combined_listings_df.Bedrooms == bedroom),:].sample(n = 75)\n",
    " \n",
    "        #Assign treatment\n",
    "        intermed_df['treatment_assignment'] = intermed_df['treatment_assignment'] = np.random.choice(treatments, replace = True, size = 75)\n",
    "        \n",
    "        #Append results to city df\n",
    "        city_df = city_df.append(intermed_df)\n",
    "        \n",
    "        #shuffle listings within city df\n",
    "        city_df = city_df.sample(frac = 1)\n",
    "        \n",
    "        #re-order columns\n",
    "        city_df = city_df[['city', 'Listing_Title', 'Posting_Date', 'Bedrooms', 'Sqft', 'Price',\\\n",
    "                           'posting_ID', 'treatment_assignment', 'full_URL', 'Reply_contact_info_link',\\\n",
    "                                         'reply_email_TO_BE_FILLED_IN', 'Apartment_Complex_Flag']]\n",
    "\n",
    "        #export each city\n",
    "        city_df.to_csv(city + ' Listings For Data Pull 3-17-17.csv', sep=',', encoding='utf-8')\n",
    "    \n",
    "    #append results to data frame\n",
    "    test_sample_df = test_sample_df.append(city_df)\n",
    "\n",
    "print test_sample_df.shape\n",
    "\n",
    "#test_sample_df.to_csv('Listings For Data Pull 3-17-17.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomization Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sqft</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chicago</th>\n",
       "      <td>1344.693750</td>\n",
       "      <td>2508.368056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>houston</th>\n",
       "      <td>1395.103111</td>\n",
       "      <td>2462.300694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sandiego</th>\n",
       "      <td>1328.134240</td>\n",
       "      <td>2503.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seattle</th>\n",
       "      <td>1229.387500</td>\n",
       "      <td>2493.336111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Sqft        Price\n",
       "city                              \n",
       "chicago   1344.693750  2508.368056\n",
       "houston   1395.103111  2462.300694\n",
       "sandiego  1328.134240  2503.183333\n",
       "seattle   1229.387500  2493.336111"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Average square footage and price for each city\n",
    "by_city_avgs = combined_listings_df.groupby(by = ['city'])['Sqft', 'Price'].mean()\n",
    "\n",
    "by_city_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot square footage and price by city\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Machine_learning_python2]",
   "language": "python",
   "name": "conda-env-Machine_learning_python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
